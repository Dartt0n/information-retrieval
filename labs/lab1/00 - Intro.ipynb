{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "95edd0c6-b44e-40d6-9e9c-48f42ddd5207",
   "metadata": {},
   "source": [
    "# 0. [OPTIONAL] Installing course dependencies\n",
    "\n",
    "These are dependencies for the whole course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "7ae1d575-f278-450f-aa6f-d99e075a79e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: '../requirements.txt'\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49m/opt/homebrew/opt/python@3.11/bin/python3.11 -m pip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158874a2-fb2f-4429-8c13-01457e9767b4",
   "metadata": {},
   "source": [
    "You may skip the next block for now. You will need `ffmpeg` on week 12."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "4f337db2-7f6a-4520-b51e-f38f064ed599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fish: Unknown command: conda\n",
      "fish: \n",
      "conda install -c conda-forge ffmpeg -y\n",
      "^~~~^\n"
     ]
    }
   ],
   "source": [
    "# !conda update -y base conda\n",
    "!conda install -c conda-forge ffmpeg -y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06c2f0dd-1d63-412c-ae93-2f012c3d8f0c",
   "metadata": {},
   "source": [
    "Run the next cell if you want to download embedding model, but this is not required during this lab. You can do it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "74fa01d2-dd4d-48d4-a27b-9ae610d9e7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/dartt0n/.rye/py/cpython@3.12.3/bin/python3: No module named spacy\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_trf_distilbertbaseuncased_lg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef7bedc-d559-437e-b106-12c860acfbab",
   "metadata": {},
   "source": [
    "# 1. Touching the Internet\n",
    "\n",
    "Solve the following task.\n",
    "1. Download [this page](https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt)\n",
    "2. Save it to the file with the **unique** name derived from the URL. NB File with another URL should not be save into the file with this name. E.g. [this file](https://github.com/IUCVLab/information-retrieval/blob/main/datasets/facts.txt) is another file with another content!\n",
    "\n",
    "Hints:\n",
    "- [requests](https://docs.python-requests.org/en/latest/) library is cool.\n",
    "- [hashlib](https://docs.python.org/3/library/hashlib.html) may help with computing hash strings.\n",
    "- when you download and save the data, don't try to encode and decode it. Use binary format when working with streams and files. <span style=\"color:red\">Discuss with your TA which encodings you know and how they differ</span>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "639fb2d7-d577-4ae6-beb4-2487213024cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file 1:  e2d9cd700ca8f8c01cd68193a1249d29030a1782108055b444a5ead7ab310425.txt\n",
      "file 2:  7770f104796093e196fac1e6822438ba9a2dadc33030edb373caf84a4fc99005.txt\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from hashlib import sha256\n",
    "\n",
    "url1 = \"https://raw.githubusercontent.com/IUCVLab/information-retrieval/main/datasets/facts.txt\"\n",
    "url2 = \"https://github.com/IUCVLab/information-retrieval/blob/main/datasets/facts.txt\"\n",
    "\n",
    "\n",
    "def save_file(url, extenstion: str = \"\") -> str:\n",
    "    filename = sha256(url.encode()).hexdigest() + extenstion\n",
    "    response = requests.get(url=url)\n",
    "    response.raise_for_status()\n",
    "\n",
    "    with open(filename, \"wb\") as f:\n",
    "        f.write(response.content)\n",
    "\n",
    "    return filename\n",
    "\n",
    "\n",
    "print(\"file 1: \", save_file(url1, \".txt\"))\n",
    "print(\"file 2: \", save_file(url2, \".txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7601aaf-3534-42cd-bb33-665d2d92c79d",
   "metadata": {},
   "source": [
    "# 2. Parsing different formats\n",
    "\n",
    "Most probably, if you meet something in the Internet, this is one of: binary, plain text, XML, or json. XML then splits into xHTML, RSS, Atom, SOAP, XML-RPC, ... . Your task is to learn, how to process different formats.\n",
    "\n",
    "## 2.1. JSON\n",
    "\n",
    "In [the given file](http://sprotasov.ru/data/postnauka.txt) there is valid json. Parse this file and print all video URLs, which have `computer science` tag. Use built-in features of `requests`, or just a `json` library ([ref](https://docs.python.org/3/library/json.html)).\n",
    "\n",
    "Hint:\n",
    "- if the file has issues with parsing read about [the difference](https://stackoverflow.com/questions/57152985/what-is-the-difference-between-utf-8-and-utf-8-sig)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "628c2c18-b896-40f9-bac9-2fefc5027da1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'title': 'Компьютерные науки: от программирования до искусственного интеллекта', 'id': '0004', 'parentId': None, 'tags': ['computer science', 'interview', 'radio', 'postnauka'], 'author': 'Stanislav Protasov', 'url': 'http://postnauka.ru/talks/31897', 'description': 'Интервью об особенностях компьютерных наук, параллельном программировании и искусственном интеллекте', 'contentType': 'article', 'image': 'http://cdn.postnauka.netdna-cdn.com/img/2014/09/Computer-Science.jpg'}\n",
      "{'title': 'Внутренняя разработка в Computer Science', 'id': '0005', 'parentId': None, 'tags': ['computer science', 'internal development', 'video', 'postnauka'], 'author': 'Stanislav Protasov', 'url': 'http://postnauka.ru/video/24306', 'description': 'О математических задачах в компьютерной науке, обработке отчетов об ошибках и тестировании ПО', 'contentType': 'video', 'image': 'http://cdn.postnauka.netdna-cdn.com/img/2014/04/Stanislav-Protasov-Vnutrennyaya-razrabotka-v-omputer-Science.jpg'}\n",
      "{'title': 'Что такое Big Data?', 'id': '0006', 'parentId': None, 'tags': ['computer science', 'big data', 'iot', 'postnauka'], 'author': 'Stanislav Protasov', 'url': 'http://postnauka.ru/faq/46974', 'description': 'Как анализ больших данных стал главной задачей в IT', 'contentType': 'article', 'image': 'http://cdn.postnauka.netdna-cdn.com/img/2015/04/Big-Data.jpg'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import requests\n",
    "import io\n",
    "\n",
    "url = \"http://sprotasov.ru/data/postnauka.txt\"\n",
    "\n",
    "# requests.get(url).json()\n",
    "# data = json.load(io.BytesIO(requests.get(url).content))  # buffered for memory optimization\n",
    "data = json.loads(requests.get(url).content)\n",
    "for entry in data:\n",
    "    if \"computer science\" in entry[\"tags\"]:\n",
    "        print(entry)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a97530a0-46d4-47e3-a7bb-ca479680007d",
   "metadata": {},
   "source": [
    "## 2.2. HTML\n",
    "\n",
    "For a given StackExchange answer extract logins of the contributors (who asked and who answered) with votes. [bs4](https://beautiful-soup-4.readthedocs.io/en/latest/) will help you to do the job.\n",
    "\n",
    "I can recommend to use CSS or XPath selectors. `div` elements with `post-layout` class represent answers. Inside there are `div` with `votecell` class stroring votes number and `div` with class `user-details` storing user info. My personal recommendation is to use `css selectors`, which are [documented here](https://beautiful-soup-4.readthedocs.io/en/latest/#css-selectors)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "e67db473-a8f0-414c-adbc-ce0e9e4710bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://math.stackexchange.com/questions/411486/understanding-the-singular-value-decomposition-svd\n",
      "['Rodrigo de Azevedo',\n",
      " 'Celdor',\n",
      " 'Ittay Weiss',\n",
      " 'Tomasz Bartkowiak',\n",
      " 'Bart Vanderbeke',\n",
      " 'Bart Vanderbeke',\n",
      " 'littleO',\n",
      " 'hgfei',\n",
      " 'TheSHETTY-Paradise']\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = (\n",
    "    \"https://math.stackexchange.com/questions/411486/\"\n",
    "    \"understanding-the-singular-value-decomposition-svd\"\n",
    ")\n",
    "print(url)\n",
    "\n",
    "# TODO. Your code here should parse HTML source page and find contributors of the repository.\n",
    "html = BeautifulSoup(requests.get(url).content)\n",
    "\n",
    "pprint([x.text for x in html.select(\".user-details > a\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6708766b-0db3-4062-87a1-9ba96c60440b",
   "metadata": {},
   "source": [
    "# 2.3. RSS feed\n",
    "\n",
    "A lot of information is already organized in typed XML documents. Podcasts, for example, are just RSS feed. Parse [the feed of this podcast](http://sprotasov.ru/podcast/rss.xml) and print out:\n",
    "- the number of episodes\n",
    "- the length of the time span between the first and the last episodes (in days).\n",
    "\n",
    "Use [`feedparser` library for this](https://waylonwalker.com/parsing-rss-python/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2090f810-d706-4bb2-8c85-d485a48432a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2030 days, 9:25:00\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from time import mktime\n",
    "import feedparser\n",
    "import jq\n",
    "from pprint import pprint\n",
    "\n",
    "rss = \"http://sprotasov.ru/podcast/rss.xml\"\n",
    "\n",
    "# TODO: complete the code to compute the time span of all the episodes.\n",
    "last, *_, first = map(lambda x: x[\"published_parsed\"], feedparser.parse(rss)[\"entries\"])\n",
    "\n",
    "last = datetime.fromtimestamp(mktime(last))\n",
    "first = datetime.fromtimestamp(mktime(first))\n",
    "\n",
    "print(last - first)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a7a63b1-c106-4a13-8e55-9240e9c8418f",
   "metadata": {},
   "source": [
    "# 3. [EXTRA TASK] Solving simple information retrieval task\n",
    "\n",
    "According to the name, `information retrieval` is the discipline, which helps retrieves information (from unstructured sources). Thus, we will retrieve some information from [this news article](https://www.bbc.com/news/world-us-canada-59944889). Your task is to write a code, which will answer the question: **How many people die every day in the US waiting for a transplant?** Write flexible enough code. Test yourself by changing the link to [this one](https://www.americantransplantfoundation.org/about-transplant/facts-and-myths/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "b2646e10",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/dartt0n/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "nltk.download(\"punkt_tab\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "7660c706-371b-4050-aede-e4b3e4014ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from nltk import sent_tokenize\n",
    "from bs4 import BeautifulSoup\n",
    "from pprint import pprint\n",
    "from Levenshtein import distance as levenshtein_distance\n",
    "\n",
    "\n",
    "url = \"https://www.bbc.com/news/world-us-canada-59944889\"\n",
    "url2 = \"https://www.americantransplantfoundation.org/about-transplant/facts-and-myths/\"\n",
    "\n",
    "question = \"How many people die every day in the US waiting for a transplant?\"\n",
    "\n",
    "\n",
    "html = BeautifulSoup(\n",
    "    requests.get(\n",
    "        url2,\n",
    "        headers={\n",
    "            # fake user agent\n",
    "            \"User-Agent\": \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.6 Safari/605.1.15\"\n",
    "        },\n",
    "    ).content\n",
    ")\n",
    "\n",
    "text = html.text\n",
    "\n",
    "while '\\n\\n' in text:\n",
    "    text = text.replace('\\n\\n', '\\n')\n",
    "\n",
    "text = text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "345be991",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_documents = sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ed62b956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- 1. On average, 16 people die every day from the lack of available organs for transplant.\n",
      "- 2. Over 104,000 people in the United States are currently on the waiting list for a lifesaving organ transplant.\n",
      "- 3. Over 850,000 transplants have occurred in the U.S. since 1988.\n",
      "- 4. There are nearly as many people dying per year of organ disease as are on the transplant waiting list currently!\n",
      "- 5. The US saw a 7.5% increase in deceased donors last year- from 13,863 in 2021 to 14,903 in 2022\n",
      "Another name is added to the national transplant waiting list every 9 minutes.\n",
      "- 6. If you are rich or a celebrity, you can move up the waiting list more quickly.\n",
      "- 7. One in three donors are not biologically related to the recipient.\n",
      "- 8. More than 44,000 corneal transplants take place each year in the United States.\n",
      "- 9. The first priority of a medical professional is to save lives when sick or injured people come to the hospital.\n",
      "- 10. After donating an organ or tissue, a closed casket funeral is the only option.\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "sentences = [x.split() for x in dataset_documents]\n",
    "terms = question.split()\n",
    "\n",
    "term_freq = defaultdict(lambda: 0.0)\n",
    "\n",
    "for index, sentence in enumerate(sentences):\n",
    "    for term in terms:\n",
    "        if term in sentence:\n",
    "            term_freq[index] += 1\n",
    "\n",
    "    term_freq[index] /= len(sentence)\n",
    "\n",
    "for rank, (index, _) in enumerate(sorted(term_freq.items(), key=lambda x: x[1], reverse=True)[:10]):\n",
    "    print(f\"- {rank+1}. {dataset_documents[index]}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
